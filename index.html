<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ce Zheng</title>
  
  <meta name="author" content="Ce Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!-- 	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ce Zheng</name>
              </p>
              <p style="font-size:16px">I am currently a Postdoctoral Fellow at the <a href="https://www.ri.cmu.edu/" style="font-size:16px">Robotics Institute</a>, <a href="https://www.cmu.edu/" style="font-size:16px">Carnegie Mellon University</a>, working under the guidance of <a href="https://www.laszlojeni.com/" style="font-size:16px">Prof. L√°szl√≥ A. Jeni</a>.
		I obtained my Ph.D. degree at <a href="https://www.crcv.ucf.edu/" style="font-size:16px">Center for Research in Computer Vision (CRCV)</a> at University of Central Florida (UCF)
		under the supervision of <a href="https://www.crcv.ucf.edu/chenchen/index.html" style="font-size:16px">Prof. Chen Chen</a>. </p>
              <p style="font-size:16px"> Before joining UCF, I obtained my Master's degree at Tufts University in Aug 2019, advised by <a href="https://facultyprofiles.tufts.edu/shuchin-aeron" style="font-size:16px">Prof. Shuchin Aeron</a> and <a href="https://facultyprofiles.tufts.edu/eric-miller" style="font-size:16px">Prof. Eric Miller</a>. 
		I received my Bachelor's Degree at University of Bridgeport and Wuhan University of Science and Technology in June 2016. 
              </p>
              <p style="text-align:center">
                <a href="mailto:cezheng@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=YFKLC58AAAAJ&hl=en#">Google Scholar</a> &nbsp/&nbsp
<!--                 <a href="https://github.com/zczcwh/">Github</a> &nbsp/&nbsp -->
		<a href="data/CV_CeZheng.pdf">Resume</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ce photo.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ce photo.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Research</heading>
        <p>
          My research interests are Computer Vision, AIGC, and Vision Language Models. Specifically, I focus on:
        </p>
        <ul>  
          <li>3D Vision, Human Pose Estimation and Mesh Recovery,</li>   
          <li>Efficient Networks,</li>    
          <li>Generative AI (Diffusion-based Generation/Synthesis),</li>    
          <li>Vision Language Models for Human Understanding,</li>   
          <li>...</li> 
        </ul>
        <p>Below is a <strong>selected</strong> list of my works. The full publication can be found on my Google Scholar page. <font color="red">If you are interested in these topics and want to work with me, please don't hesitate to reach out to me via email.</font></p> 
      </td>
    </tr>
  </tbody>
</table>
	      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work/Internship Experience</heading>
            </td>
          </tr>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/innopeak.jpg' width="110">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Intern</strong>
                <br> Innopeak Tech, Seattle, USA. Summer 2022
                <br> Mentor: <a href="http://maple-lab.net/gqi/">Guo-Jun Qi</a>,
                <p></p>
                <p> Human mesh recovery for single images.</p>
              </td>
            </tr>
          </tr>
          </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Recent News</heading>
            <!-- <p>In reverse chronological order:</p> -->
            <ul>
	      <li>
                    <strong>2025-02:</strong> Our paper about In-Bed Human Mesh Recovery has been accepted by CVPR 2025!
	      <li>
		    <strong>2025-02:</strong> We are organizing <a href="https://beam-workshop2025.github.io/" style="font-size:16px">The First Workshop on Benchmarking and Expanding AI Multimodal Approaches at CVPR 2025</a>.!
	      <li>
                    <strong>2024-04:</strong> I joined the Robotics Institute, Carnegie Mellon University as a Postdoctoral Fellow!
	      <li>
                    <strong>2024-02:</strong> Two papers (MvACon and Domain Generalization 3D HPE) are accepted by CVPR 2024!
	      <li>
                    <strong>2024-02:</strong> My PhD dissertation won Outstanding Dissertation Award (university-wide)!
	      <li>
                    <strong>2023-12:</strong> I pass my PhD dissertation defense!
              <li>
                    <strong>2023-09:</strong> One paper (Context-aware PoseFormer) has been accepted by NeurIPS 2023!
	      <li>
                    <strong>2023-07:</strong> Two papers (MonoXiver and Source-free DA HPE) have been accepted by ICCV 2023!
              <li>
                    <strong>2023-05:</strong> Our human pose estimation survey paper has been accepted by ACM Computing Surveys (IF=14.3)!
              <li>
                    <strong>2023-02:</strong> Three papers (FeatER, POTTER, PoseFormerV2) have been accepted by CVPR 2023!             
                     
            <ul>

            </p>
          </td>
        </tr>
		    

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Published papers</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


	 <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/POTTER.JPG' width="160"></div> -->
                <img src='images/diffmesh_demo.gif' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2303.13397.pdf">
                <papertitle> DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery from Videos </papertitle>
              </a>
              <br>
              <strong>Ce Zheng</strong>,
	      <a href="https://xianpeng919.github.io/">Xianpeng Liu</a>,
              <a href="https://scholar.google.com/citations?user=IFqidw4AAAAJ&hl=en">Qucheng Peng</a>,
	      <a href="https://research.ece.ncsu.edu/ivmcl/">Tianfu Wu</a>,
	      <a href="https://webpages.charlotte.edu/pwang13/">Pu Wang</a>,    
              <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>.
              <br>
              <em> IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025
              <br>
              <a href="https://arxiv.org/abs/2303.13397">paper</a> &nbsp/&nbsp
              <a href="https://zczcwh.github.io/diffmesh_page/">project page</a>
              <p></p>
              <p>
                A Diffusion-Driven Transformer-based Framework to decode specific motion patterns from the input sequence.
              </p>
            </td>
          </tr>

	 <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/POTTER.JPG' width="160"></div> -->
                <img src='images/freqmix.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2407.12322">
                <papertitle> Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer </papertitle>
              </a>
              <br>
	      <a href="https://sites.google.com/view/wenhanwu/">Wenhan Wu</a>,
              <strong>Ce Zheng</strong>,
	      <a href="">Zihao Yang</a>,
              <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>,
	      <a href="https://srijandas07.github.io/">Srijan Das</a>,
              <a href="https://cci.charlotte.edu/directory/aidong-lu/">Aidong Lu</a>.
              <br>
              <em> ACM Multimedia (ACM MM), 2024 
              <br>
              <a href="https://arxiv.org/pdf/2407.12322">paper</a> &nbsp/&nbsp
              <a href="">project page</a>
              <p></p>
              <p>
                Frequency-aware Mixed Transformer (FreqMixFormer) for recognizing similar skeletal actions with subtle discriminative motions.
              </p>
            </td>
          </tr>
		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/capose.jpg' width="160"></div> -->
                <img src='images/VITA.png' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>VITA: ViT Acceleration for Efficient 3D Human Mesh Recovery via Hardware-Algorithm Co-Design</papertitle>
              </a>
              <br>
	      Shilin Tian,
	      Chase Szafranski,
              <strong>Ce Zheng</strong>, 
	      Fan Yao, 
	      Ahmed Louri,
	      <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>,
	      <a href="http://haozheng.us/">Hao Zheng</a>.	    
              <br>
              <em> Design Automation Conference(<b>DAC</b>), 2024
              <br>
              <a href="https://www.crcv.ucf.edu/chenchen/2024_DAC_VITA_Final.pdf">paper</a> &nbsp/&nbsp
              <p></p>
              <p>
               VITA, a hardware and algorithm co-design framework for ViT-based HMR with improved performance and energy efficiency
              </p>
            </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/capose.jpg' width="160"></div> -->
                <img src='images/DG.png' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation</papertitle>
              </a>
              <br>
	      Qucheng Peng,
              <strong>Ce Zheng</strong>, 
	      <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>.	    
              <br>
              <em> IEEE Conference on Computer Vision and Pattern Recognition(<b>CVPR</b>), 2024
              <br>
              <a href="https://arxiv.org/abs/2403.11310">paper</a> &nbsp/&nbsp
	      <a href="https://github.com/davidpengucf/DAF-DG">code</a> &nbsp/&nbsp
              <p></p>
              <p>
               we propose a novel dual-augmentor framework designed to enhance domain generalization in 3D human pose estimation.
              </p>
            </td>
	</tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/capose.jpg' width="160"></div> -->
                <img src='images/detection.png' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Multi-View Attentive Contextualization for Multi-View 3D Object Detection</papertitle>
              </a>
              <br>
	      <a href="https://xianpeng919.github.io/">Xianpeng Liu</a>,
              <strong>Ce Zheng</strong>,
	      Ming Qian,
              <a href="https://xuenan.net/">Nan Xue</a>, 
	      <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>,
	      Zhebin Zhang,
	      Chen Li,
	      <a href="https://research.ece.ncsu.edu/ivmcl/">Tianfu Wu</a>.
              <br>
              <em> IEEE Conference on Computer Vision and Pattern Recognition(<b>CVPR</b>), 2024
              <br>
              <a href="https://arxiv.org/abs/2405.12200">paper</a> &nbsp/&nbsp
	      <a href="https://xianpeng919.github.io/mvacon/">Project page</a> &nbsp/&nbsp
              <p></p>
              <p>
                Multi-View Attentive Contextualization (MvACon), a simple yet effective method for improving 2D-to-3D feature lifting in query-based multi-view 3D (MV3D) object detection.
              </p>
            </td>
	</tr>
		
        <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/capose.jpg' width="160"></div> -->
                <img src='images/capose.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://qitaozhao.github.io/ContextAware-PoseFormer">
                <papertitle>A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation</papertitle>
              </a>
              <br>
	      <a href="https://qitaozhao.github.io/">Qitao Zhao</a>
              <strong>Ce Zheng</strong>, 
              <a href="https://humanperception.github.io/">Mengyuan Liu</a>,
	      <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>.	    
              <br>
              <em> Thirty-seventh Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2023
              <br>
              <a href="https://arxiv.org/pdf/2311.03312.pdf">paper</a> &nbsp/&nbsp
              <a href="https://qitaozhao.github.io/ContextAware-PoseFormer">project page</a>
              <p></p>
              <p>
                We revisit the 2D-3D lifting pipeline, leveraging the readily available intermediate visual representations.
              </p>
            </td>
	</tr>

		
  
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/MonoXiver.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2304.01289.pdf">
                <papertitle>Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver </papertitle>
              </a>
              <br>
	      <a href="https://xianpeng919.github.io/">Xianpeng Liu</a>,
              <strong>Ce Zheng</strong>,
	      Kelvin Cheng,
              <a href="https://xuenan.net/">Nan Xue</a>, 
              <a href="http://maple-lab.net/gqi/">Guo-Jun Qi</a>,
	      <a href="https://research.ece.ncsu.edu/ivmcl/">Tianfu Wu</a>.
              <br>
              <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2304.01289.pdf">paper</a> &nbsp/&nbsp
	      <a href="">Code</a>
              <p></p>
              <p>
                MonoXiver: leverages the self-attention mechanism for proposal verification, and ultimately delivers high-quality 3D box predictions.
              </p>
            </td>
          </tr>	

	 <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/source-free.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2308.03202.pdf">
                <papertitle>Source-free Domain Adaptive Human Pose Estimation</papertitle>
              </a>
              <br>
	      <a href="https://davidpengiupui.github.io/">Qucheng Peng</a>,
              <strong>Ce Zheng</strong>,
	      <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>.
              <br>
              <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2308.03202.pdf">paper</a> &nbsp/&nbsp
	      <a href="https://github.com/davidpengucf/SFDAHPE">Code</a>
              <p></p>
              <p>
               we propose source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process.
              </p>
            </td>
          </tr>	

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/poster.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2204.04083.pdf">
                <papertitle>POSTER: A Pyramid Cross-Fusion Transformer Network for Facial Expression Recognition</papertitle>
              </a>
              <br>
              <strong>Ce Zheng</strong>,
	      <a href="https://mmendiet.github.io/">Matias Mendieta</a>,     
              <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>.
              <br>
              <em>ICCV Workshop, 2023
              <br>
              <a href="https://arxiv.org/pdf/2204.04083.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/zczcwh/POSTER">code</a>
              <p></p>
              <p>
                 we propose a two-stream Pyramid crOss-fuSion TransformER network (POSTER) for facial expression recognition.
              </p>
            </td>
          </tr>

	          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/POTTER.JPG' width="160"></div> -->
                <img src='images/potter.gif' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.13357.pdf">
                <papertitle>POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery</papertitle>
              </a>
              <br>
              <strong>Ce Zheng</strong>,
              <a href="https://xianpeng919.github.io/">Xianpeng Liu</a>,
              <a href="http://maple-lab.net/gqi/">Guo-Jun Qi</a>,
              <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>.
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2303.13357.pdf">paper</a> &nbsp/&nbsp
              <a href="https://zczcwh.github.io/potter_page/">project page</a>
              <p></p>
              <p>
                A lightweight pure transformer architecture named POoling aTtention TransformER (POTTER) for the HMR task from single images.
              </p>
            </td>
          </tr>


          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/feater.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
		  
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2205.15448.pdf">
                <papertitle>FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER</papertitle>
              </a>
              <br>
              <strong>Ce Zheng</strong>, 
              <a href="https://mmendiet.github.io/">Matias Mendieta</a>,
              <a href="https://taoyang1122.github.io/">Taojiannan Yang</a>,
              <a href="http://maple-lab.net/gqi/">Guo-Jun Qi</a>, 
              <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>.
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2205.15448.pdf">paper</a> &nbsp/&nbsp
              <a href="https://zczcwh.github.io/feater_page/">project page</a>
              <p></p>
              <p>
                An efficient transformer-based method for human pose estimation and mesh reconstruction.
              </p>
            </td>
	</tr>

	        </tr>
          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/poseformerv2.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.17472.pdf">
                <papertitle>PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation</papertitle>
              </a>
              <br>
	      <a href="https://qitaozhao.github.io/">Qitao Zhao</a>, 
              <strong>Ce Zheng</strong>, 
              <a href="https://humanperception.github.io/">Mengyuan Liu</a>, 
              <a href="https://wangpichao.github.io/">Pichao Wang</a>,
              <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>.
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2303.17472.pdf">paper</a> &nbsp/&nbsp
              <a href="https://qitaozhao.github.io/PoseFormerV2">project page</a>
              <p></p>
              <p>
                An extension of our PoseFormer paper to improve robustness.
              </p>
            </td>
          </tr>

		
	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/IJCAI-SSL.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2305.00666.pdf">
                <papertitle>Part Aware Contrastive Learning for Self-Supervised Action Recognition </papertitle>
              </a>
              <br>
	      Yilei Hua,
	      <a href="https://sites.google.com/view/wenhanwu/">Wenhan Wu</a>,
              <strong>Ce Zheng</strong>,
	      Aidong Lu,
              <a href="https://humanperception.github.io/">Mengyuan Liu</a>, 
              <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>,
	      Shiqian Wu.
              <br>
              <em> The International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>) </em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2305.00666.pdf">paper</a> &nbsp/&nbsp
	      <a href="https://github.com/GitHubOfHyl97/SkeAttnCLR">code</a>
              <p></p>
              <p>
               SkeAttnCLR: An attention-based contrastive learning framework for skeleton representation learning.
              </p>
            </td>
          </tr>	

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/LAMP.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2307.11934.pdf">
                <papertitle>LAMP: Leveraging Language Prompts for Multi-person Pose Estimation </papertitle>
              </a>
              <br>
	      <a href="https://shengnanhu.netlify.app/">Shengnan Hu</a>,
              <strong>Ce Zheng</strong>,
	      <a href="https://edwardzhou130.github.io/">Zixiang Zhou</a>,
              <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>,
	      <a href="https://www.eecs.ucf.edu/~gitars/">Gita Sukthankar</a>.
              <br>
              <em> The IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>) </em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2307.11934.pdf">paper</a> &nbsp/&nbsp
	      <a href="https://github.com/shengnanh20/LAMP">code</a>
              <p></p>
              <p>
                An end-to-end pipeline that leverages both instance and joint cues from the language model for occluded pose estimation.
              </p>
            </td>
          </tr>	
		
          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/gtrs.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2111.12696.pdf">
                <papertitle>A Lightweight Graph Transformer Network for Human Mesh Reconstruction from 2D Human Pose</papertitle>
              </a>
              <br>
              <strong>Ce Zheng</strong>,
	      <a href="https://mmendiet.github.io/">Matias Mendieta</a>,
	      Pu Wang,
              Aidong Lu,   
              <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>.
              <br>
              <em> ACM International Conference on Multimedia(<strong>ACM MM</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2111.12696.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/zczcwh/GTRS">code</a>
              <p></p>
              <p>
                 A lightweight pose-based method that can reconstruct human mesh from 2D human pose
              </p>
            </td>
          </tr>
		
          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/poseformer.gif' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2103.10455.pdf">
                <papertitle>3D Human Pose Estimation with Spatial and Temporal Transformers </papertitle>
              </a>
              <br>
              <strong>Ce Zheng</strong>, 
              <a href="https://jeff-zilence.github.io/">Sijie Zhu</a>, 
              <a href="https://mmendiet.github.io/">Matias Mendieta</a>,
              <a href="https://taoyang1122.github.io/">Taojiannan Yang</a>,
              <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>, 
              <a href="https://www.cs.tulane.edu/~zding1/">Zhengming Ding</a>.
              <br>
              <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2103.10455.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/zczcwh/PoseFormer">code</a>
              <p></p>
              <p>
                PoseFormer: a spatial-temporal transformer (the first transformer-based) structure for 3D human pose estimation in videos.
              </p>
            </td>
          </tr>

	<tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/vehicleReID.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9647974">
                <papertitle>Exploiting Multi-view Part-wise Correlation via an Efficient Transformer for Vehicle Re-Identification </papertitle>
              </a>
              <br>
	      <a href="https://ming1993li.github.io/">Ming Li</a>,
	      <a href="https://people.sutd.edu.sg/~jun_liu/">Jun Liu</a>,
              <strong>Ce Zheng</strong>,
              Xinming Huang, 
	      <a href="https://zhang-vislab.github.io/">Ziming Zhang</a>.
              <br>
              <em> IEEE Transactions on Multimedia (<strong>TMM</strong>) </em>, 2021
              <br>
              <a href="https://ieeexplore.ieee.org/document/9647974">paper</a> &nbsp/&nbsp
              <p></p>
              <p>
                The first transformer-driven framework to capture comprehensive instance codes from multiple view images for vehicle ReID.
              </p>
            </td>
          </tr>		
		
          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/survey.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2012.13392.pdf">
                <papertitle>Deep Learning-Based Human Pose Estimation: A Survey </papertitle>
              </a>
              <br>
              <strong>Ce Zheng*</strong>,
              <a href="https://sites.google.com/view/wenhanwu/">Wenhan Wu</a>*,
	      <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>, 
	      <a href="https://taoyang1122.github.io/">Taojiannan Yang</a>,
              <a href="https://jeff-zilence.github.io/">Sijie Zhu</a>,
              Ju Shen,
              Nasser Kehtarnavaz,
              <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>.
              <br>
              <em> (<strong>ACM Computing Surveys, IF=14.32 </strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2012.13392.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/zczcwh/DL-HPE">Porject page</a>
              <p></p>
              <p>
                A comprehensive survey for 2D and 3D Human Pose Estimation.
              </p>
            </td>
          </tr>
		
          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/lodonet.jpg' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2009.00164.pdf">
                <papertitle>LodoNet: A Deep Neural Network with 2D Keypoint Matching for 3D LiDAR Odometry Estimation </papertitle>
              </a>
              <br>
              <strong>Ce Zheng</strong>,
              Yecheng Lyu,
	      <a href="https://ming1993li.github.io/">Ming Li</a>, 
	      <a href="https://zhang-vislab.github.io/">Ziming Zhang</a>.
              <br>
              <em> ACM International Conference on Multimedia(<strong>ACM MM</strong>)</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2009.00164.pdf">paper</a> &nbsp/&nbsp
              <p></p>
              <p>
                A new approach that extracts the matched 2D keypoint pairs(MKPs) for 3D LiDAR Odometry.
              </p>
            </td>
          </tr>	
		

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading> Under-review papers </heading>
            </td>
          </tr>
        </tbody></table>
		
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
		
         <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/POTTER.JPG' width="160"></div> -->
                <img src='images/SignLLM.png' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.13397.pdf">
                <papertitle> SignLLM: Sign Languages Production Large Language Models </papertitle>
              </a>
              <br>
	      <a href="https://signllm.github.io/">Sen Fang</a>,
	      <a href="https://leiwangr.github.io//">Lei Wang</a>,      
              <strong>Ce Zheng</strong>,
              <a href="https://www.yapengtian.com/">Yapeng Tian</a>,
              <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>.
              <br>
              <em> arXiv, 2024
              <br>
              <a href="https://arxiv.org/abs/2405.10718v1">paper</a> &nbsp/&nbsp
              <a href="https://signllm.github.io/">project page</a>
              <p></p>
              <p>
                we propose SignLLM, the first multilingual Sign Language Production (SLP) model, which includes two novel multilingual SLP modes that allow for the generation of sign language gestures from input text or prompt.
              </p>
            </td>
          </tr> 
		
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:16px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              Reviewer: TPAMI, IJCV, TIP, TCSVT, CVIU, TNNLS, Neurocomputing, Neural Networks
              <br>
              Reviewer: CVPR, ICCV, NeurIPS, ICLR, SIGGRAPH Asia, ACM MM, 
            </td>
          </tr>
        </tbody></table>
		
<!-- 	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Miscellaneous</heading>
		    
              <a href="https://www.easycounter.com/">
              <img src="https://www.easycounter.com/counter.php?zczcwh" ,="" width="10%"
              border="0" alt="Web Site Hit Counter"></a><a href="https://www.easycounter.com/">    unique visitors since Mar 2023</a>
            </td>
          </tr>
        </tbody></table>				 -->
					

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td> 
    </tr>
  </table>
</body>

</html>
