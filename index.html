<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Taojiannan Yang</title>
  
  <meta name="author" content="Taojiannan Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Taojiannan Yang</name>
              </p>
              <p style="font-size:15px">I am a fifth year PhD student in the Center for Research in Computer Vision (CRCV), University of Central Florida (UCF), advised by <a href="https://www.crcv.ucf.edu/chenchen/index.html" style="font-size:15px">Prof. Chen Chen</a>. Before that, 
                I received my bachelor's degree from <a href="https://www.ustc.edu.cn/" style="font-size:15px">University of Science and Technology of China (USTC)</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:taoyang1122@knights.ucf.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Z_--q5UAAAAJ&hl=en#">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/taoyang1122">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/taojiannan-yang-528706201/">LinkedIn</a>
              </p>
              <p style="font-size:17px">
                <font color="red">I am looking for full-time positions starting from Fall 2023.</font>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/tao photo.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/tao.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/aws.jpg' width="110">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Applied Scientist Intern</strong>
                <br> AWS AI Labs, Santa Clara, USA. Summer 2022
                <br> Host: <a href="https://bryanyzhu.github.io/">Yi Zhu</a>,
                <a href="https://www.amazon.science/author/yusheng-xie">Yusheng Xie</a>,
                <a href="https://www.astonzhang.com/">Aston Zhang</a>,
                <a href="https://scholar.google.com/citations?user=Z_WrhK8AAAAJ&hl=en">Mu Li</a>
                <p></p>
                <p> Adapt image models for efficient video understanding.</p>
              </td>
            </tr>
          </tr>
          </table>

          <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/bytedance.jpg' width="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Intern</strong>
                <br> ByteDance Inc., Mountain View, USA. Summer 2021
                <br> Host: <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang</a>,
                <a href="https://scholar.google.com.sg/citations?user=OEZ816YAAAAJ&hl=en">Xiaojie Jin</a>
                <p></p>
                <p>Efficient neural architecture search.</p>
              </td>
            </tr>
          </tr>
          </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I have a broad interest in deep learning and computer vision. My current research mainly focuses on large model adaptation, efficient neural networks, runtime adaptive networks, representation learning and its applications on image and video understanding.
                Below is a selected list of my works.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/AIM.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2302.03024.pdf">
                <papertitle>AIM: Adapting Image Models for Efficient Video Action Recognition</papertitle>
              </a>
              <br>
              <strong>Taojiannan Yang</strong>,
              Yi Zhu,
              Yusheng Xie,
              Aston Zhang,
              Chen Chen,
              Mu Li.
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2302.03024.pdf">paper</a> &nbsp/&nbsp
              <a href="https://adapt-image-models.github.io/">project</a> &nbsp/&nbsp
              <a href="https://github.com/taoyang1122/adapt-image-models">code</a>
              <p></p>
              <p>
                How to efficiently and effectively adapt image models for video understanding.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/efficientNAS.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2211.08666.pdf">
                <papertitle>Revisiting Training-free NAS Metrics: An Efficient Training-based Method</papertitle>
              </a>
              <br>
              <strong>Taojiannan Yang</strong>,
              Linjie Yang,
              Xiaojie Jin,
              Chen Chen.
              <br>
              <em>Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2211.08666.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/taoyang1122/Revisit_TrainingFree_NAS">code</a>
              <p></p>
              <p>
                Training-free metrics are highly correlated with #params and we propose a new efficient training-based method to address the problem.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/fedalign.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2111.14213.pdf">
                <papertitle>Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning</papertitle>
              </a>
              <br>
              Matias Mendieta,
              <strong>Taojiannan Yang</strong>,
              Pu Wang, 
              Minwoo Lee, 
              Zhengming Ding, 
              Chen Chen.
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022 
              <br><em><strong><font color="red">(Best Paper Finalist, 33 out of 8161)</font></strong></em>
              <br>
              <a href="https://arxiv.org/pdf/2111.14213.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/mmendiet/FedAlign">code</a>
              <p></p>
              <p>
                GradAug alleviates data heterogeneity in federated learning by smoothing loss landscape. We further improve its efficiency by proposing a new method FedAlign.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/MutualNet-tpami.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2105.07085.pdf">
                <papertitle>MutualNet: Adaptive ConvNet via Mutual Learning from Different Model Configurations</papertitle>
              </a>
              <br>
              <strong>Taojiannan Yang</strong>,
              Sijie Zhu, 
              Matias Mendieta, 
              Pu Wang, 
              Ravikumar Balakrishnan, 
              Minwoo Lee, 
              Tao Han, 
              Mubarak Shah, 
              Chen Chen.
              <br>
              <em>IEEE Transaction on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2105.07085.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/taoyang1122/MutualNet">code</a>
              <p></p>
              <p>
                We extend MutualNet to learn adaptive video models and conduct more analyses.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/gradaug.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2006.07989.pdf">
                <papertitle>GradAug: A New Regularization Method for Deep Neural Networks </papertitle>
              </a>
              <br>
              <strong>Taojiannan Yang</strong>,
              Sijie Zhu,
              Chen Chen.
              <br>
              <em>Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2006.07989.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/taoyang1122/GradAug">code</a>
              <p></p>
              <p>
                A well-generalized network should make predictions consistent with its subnetworks given differently augmented samples.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/mutualnet.png' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460290.pdf">
                <papertitle>MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution </papertitle>
              </a>
              <br>
              <strong>Taojiannan Yang</strong>,
              Sijie Zhu,
              Chen Chen,
              Shen Yan, 
              Mi Zhang, 
              Andrew Willis.
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
              <br><em><strong><font color="red">(Oral Presentation, 104 out of 5205)</font></strong></em>
              <br>
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460290.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/taoyang1122/MutualNet">code</a>
              <p></p>
              <p>
                We learn networks that can run at different widths and resolutions to meet different resource budegets during runtime.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/FedPEFT.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2210.01708.pdf">
                <papertitle>Conquering the Communication Constraints to Enable Large Pre-Trained Models in Federated Learning</papertitle>
              </a>
              <br>
              Guangyu Sun, 
              Matias Mendieta,
              <strong>Taojiannan Yang</strong>,
              Chen Chen.
              <br>
              <em>arXiv</em>, 2022
              <br>
              <!-- <a href="https://arxiv.org/pdf/2210.01708.pdf">Paper</a> -->
              <p></p>
              <p>
                We systematically study the effectiveness of parameter-efficient finetuning techniques in federated learning.
              </p>
            </td>
          </tr>	

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/feater.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2205.15448.pdf">
                <papertitle>FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER</papertitle>
              </a>
              <br>
              Ce Zheng, 
              Matias, Mendieta, 
              <strong>Taojiannan Yang</strong>,
              Guojun Qi, 
              Chen Chen.
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2205.15448.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/zczcwh/FeatER">code</a>
              <p></p>
              <p>
                An efficent method for human pose estimation and mesh reconstruction.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/poseformer.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2103.10455.pdf">
                <papertitle>3D Human Pose Estimation with Spatial and Temporal Transformers </papertitle>
              </a>
              <br>
              Ce Zheng, 
              Sijie Zhu, 
              Matias Mendieta,
              <strong>Taojiannan Yang</strong>,
              Chen Chen, 
              Zhengming Ding.
              <br>
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2103.10455.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/zczcwh/PoseFormer">code</a>
              <p></p>
              <p>
                A spatial-temporal transformer structure for 3D human pose estimation.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/visualmetriclearning.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1909.12977.pdf">
                <papertitle>Visual Explanation for Deep Metric Learning</papertitle>
              </a>
              <br>
              Sijie Zhu,
              <strong>Taojiannan Yang</strong>,
              Chen Chen.
              <br>
              <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/1909.12977.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/Jeff-Zilence/Explain_Metric_Learning">code</a>
              <p></p>
              <p>
                We visualize point-to-point activation intensity between two images.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/vigor.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2011.12172.pdf">
                <papertitle>VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval </papertitle>
              </a>
              <br>
              Sijie Zhu, 
              <strong>Taojiannan Yang</strong>,
              Chen Chen.
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2011.12172.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/Jeff-Zilence/VIGOR">code</a>
              <p></p>
              <p>
                A new benchmark for more realistic cross-view image geo-localization.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/longtailuavdetection.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2011.03822.pdf">
                <papertitle>Towards Resolving the Challenge of Long-tail Distribution in UAV Images for Object Detection </papertitle>
              </a>
              <br>
              Weiping Yu*,
              <strong>Taojiannan Yang*</strong>,
              Chen Chen.
              <br>
              <em>Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2011.03822.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/we1pingyu/DSHNet">code</a>
              <p></p>
              <p>
                We point out the long-tail distribution problem in UAV images and propose a new method to address it.
              </p>
            </td>
          </tr>
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, IEEE Transaction on Pattern Analysis and Machine Intelligence</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, IEEE Transaction on Image Processing</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, CVPR 2022, 2023</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ECCV 2022</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ICCV 2021, 2023</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, NeurIPS 2021, 2022</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ICLR 2022, 2023</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ICML 2022</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Volunteer, NeurIPS 2020</a>
            </td>
          </tr>

		
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
